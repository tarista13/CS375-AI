{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## PyTorch Warmup","metadata":{}},{"cell_type":"markdown","source":"[PyTorch](https://pytorch.org/) is the open-source machine learning framework that we'll be using in this class. It has a wide range of functionality; for now we'll just get started with some of its very basic array-processing functionality.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:22:45.646956Z","iopub.execute_input":"2024-01-26T14:22:45.647336Z","iopub.status.idle":"2024-01-26T14:22:48.861668Z","shell.execute_reply.started":"2024-01-26T14:22:45.647303Z","shell.execute_reply":"2024-01-26T14:22:48.860433Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Dot Products\n\nThe most common basic primitive in a neural network is a *linear* layer (you'll sometimes see it called a \"Dense\" layer). These are where almost all of the parameters go in a network. (Some architectures use a variant called a *convolutional* layer.) At its core, a linear layer does a bunch of *dot product*s between its *input* vector and its (learned) *weight* vectors.\n\nA few intuitions to understand what a dot product is:\n\n1. It measures *similarity*, in the sense of *alignment*. The following statements loosely capture it:\n    - \"How much does the input look like *this*?\"\n    - \"How big is the input in *this* direction?\"\n    - \"How aligned is the input with this direction?\"\n    - \"What's the cosine of the angle between the input vector and this vector?\"\n2. A bunch of dot products all together (like in a Linear layer) *rotates and stretches* the input space, like moving a camera around a scene.\n3. It's how a multiple linear regression computes its output: a weighted mixture of each part of its input.\n\nRecall that we can make a line by an expression like `y = w*x + b`. (Some of you may remember *mx+b* , but we'll use *w* for the *weight(s)* instead.)\n\nThat's a multiplication followed by a sum. We can extend that to lots of *x*'s, each of which needs a corresponding *w*:\n\n`y = w1*x1 + w2*x2 + ... + wN*xN + b`\n\nFor simplicity, let's start by ignoring the `b`ias.  So we're left with\n\n`y = w1*x1 + w2*x2 + ... + wN*xN`\n\nthat is, multiply each number in `w` by its corresponding number in `x` and add up the results: `sum(w[i] * x[i] for i in range(N))`. Or, in mathematical notation: $\\sum_{i=1}^{N} w_i x_i.$\n\nThe result is called a *dot product*, and is one of the fundamental operations in linear algebra. At this point you don't need to understand all the linear algebra part of this, we're just implementing a common calculation.\n\nLet's do that in pure Python, and then in PyTorch. To start, let's make a `w`eights and an `x`.","metadata":{}},{"cell_type":"code","source":"w = torch.tensor([2.0, -1.0])\nw","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:35:07.743130Z","iopub.execute_input":"2024-01-26T14:35:07.743495Z","iopub.status.idle":"2024-01-26T14:35:07.751950Z","shell.execute_reply.started":"2024-01-26T14:35:07.743465Z","shell.execute_reply":"2024-01-26T14:35:07.750857Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([ 2., -1.])"},"metadata":{}}]},{"cell_type":"code","source":"x = torch.tensor([1.5, -3.0])\nx","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:35:09.460015Z","iopub.execute_input":"2024-01-26T14:35:09.460719Z","iopub.status.idle":"2024-01-26T14:35:09.466781Z","shell.execute_reply.started":"2024-01-26T14:35:09.460684Z","shell.execute_reply":"2024-01-26T14:35:09.466047Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"tensor([ 1.5000, -3.0000])"},"metadata":{}}]},{"cell_type":"markdown","source":"The shapes of `w` and `x` must match.","metadata":{}},{"cell_type":"code","source":"N = len(w)\nassert N == len(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:35:11.046024Z","iopub.execute_input":"2024-01-26T14:35:11.046393Z","iopub.status.idle":"2024-01-26T14:35:11.051290Z","shell.execute_reply.started":"2024-01-26T14:35:11.046364Z","shell.execute_reply":"2024-01-26T14:35:11.050280Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### `for` loop approach\n\n**Task**: *Write a function that uses a `for` loop* to compute the dot product of `w` and `x`. Name the function `dot_loop`. Check that you get `6.0` for the `w` and `x` provided in the template.","metadata":{}},{"cell_type":"code","source":"def dot_loop(w, x):\n    l = len(w)\n    for i in range(l):\n        return sum(w * x)\ndot_loop(w, x)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:52:42.732468Z","iopub.execute_input":"2024-01-26T14:52:42.732846Z","iopub.status.idle":"2024-01-26T14:52:42.741353Z","shell.execute_reply.started":"2024-01-26T14:52:42.732816Z","shell.execute_reply":"2024-01-26T14:52:42.740336Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor(6.)"},"metadata":{}}]},{"cell_type":"markdown","source":"Here are some test cases that `dot_loop` should pass. You don't need to understand how this code works yet, but it would reward some study. (Note that, like most tests, if it passes you'll see no output when the cell runs.)","metadata":{}},{"cell_type":"code","source":"test_cases = [\n    ([0.], [500.], 0.0),\n    ([1., 0.0], [50.0, .5], 50.0),\n    ([-1.0, 1.0], [-1.0, 1.0], 2.0)\n]\ndef run_dot_tests(f):\n    assert all(\n        torch.isclose(\n            f(torch.tensor(w), torch.tensor(x)),\n            torch.tensor(prod))\n        for w, x, prod in test_cases)\nrun_dot_tests(dot_loop)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:52:46.299377Z","iopub.execute_input":"2024-01-26T14:52:46.299757Z","iopub.status.idle":"2024-01-26T14:52:46.307877Z","shell.execute_reply.started":"2024-01-26T14:52:46.299728Z","shell.execute_reply":"2024-01-26T14:52:46.306557Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"#### Torch Elementwise Operations\n\nBut that's a lot of typing for a concept that we're going to use very frequently. To shorten it (and make it run way faster too!), we'll start taking advantage of some of Torch's builtin functionality.\n\nFirst, we'll learn about *elementwise operations* (called *pointwise operations* in the [PyTorch docs](https://pytorch.org/docs/stable/torch.html#pointwise-ops)).\n\nIf you try to `*` two Python lists together, you get a `TypeError` (how do you multiply lists??). But in PyTorch (and NumPy, which it's heavily based on), array operations happen *element-by-element* (sometimes called *elementwise*): to multiply two tensors that have the same shape, multiply each number in the first tensor with the corresponding number of the second tensor. The result is a new tensor of the same shape with all the elementwise products.\n\n**Task**: Predict what you'll get from running `w * x`. Then try it and compare with your prediction. (No need to write an explanation here.)","metadata":{}},{"cell_type":"code","source":"# Predict that this will output a tensor of 3,3\nw * x","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:52:50.106547Z","iopub.execute_input":"2024-01-26T14:52:50.107549Z","iopub.status.idle":"2024-01-26T14:52:50.113810Z","shell.execute_reply.started":"2024-01-26T14:52:50.107511Z","shell.execute_reply":"2024-01-26T14:52:50.112751Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"tensor([3., 3.])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Torch Reduction Ops\n\nTorch also provides [*reduction* methods](https://pytorch.org/docs/stable/torch.html#reduction-ops), so named because they *reduce* the number of elements in a Tensor.\n\nOne really useful reduction op is `.sum`. (I also frequently use `.mean`, `.max`, and `.argmax`).\n\n**Task**: Predict the output of running `x.sum()` Then try it and compare with your prediction.\n\n> You can also write that as `torch.sum(w)`.","metadata":{}},{"cell_type":"code","source":"# I predict that the output of running this command will be 1 because there is only 1 tensor\ntorch.sum(w)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:52:53.161916Z","iopub.execute_input":"2024-01-26T14:52:53.162705Z","iopub.status.idle":"2024-01-26T14:52:53.169552Z","shell.execute_reply.started":"2024-01-26T14:52:53.162666Z","shell.execute_reply":"2024-01-26T14:52:53.168745Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"tensor(1.)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Building a dot product out of Torch ops\n\nNow **make a new version of `dot_loop`, called `dot_ops`**, that uses an elementwise op to multiply corresponding numbers and a reduction op to sum the result. Check that the result still passes the tests.","metadata":{}},{"cell_type":"code","source":"def dot_ops(w, x):\n    return torch.sum(torch.mul(w,x))\ndot_ops(w, x)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:52:55.102742Z","iopub.execute_input":"2024-01-26T14:52:55.103130Z","iopub.status.idle":"2024-01-26T14:52:55.111466Z","shell.execute_reply.started":"2024-01-26T14:52:55.103099Z","shell.execute_reply":"2024-01-26T14:52:55.110440Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"tensor(6.)"},"metadata":{}}]},{"cell_type":"code","source":"run_dot_tests(dot_ops)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:52:59.184097Z","iopub.execute_input":"2024-01-26T14:52:59.185324Z","iopub.status.idle":"2024-01-26T14:52:59.191095Z","shell.execute_reply.started":"2024-01-26T14:52:59.185279Z","shell.execute_reply":"2024-01-26T14:52:59.189718Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"\nFinally, since `dot` is such an important operation, PyTorch provides it directly:\n\n```python\ntorch.dot(w, x)\n```\n\nPython recently introduced a \"matmul operator\", `@`, that does the same thing.\n\n```python\nw @ x\n```\n\nTo apply this knowledge, let's try writing a slightly more complex function: a linear transformation layer.\n","metadata":{}},{"cell_type":"markdown","source":"## Linear Layer\n\nThe most basic component of a neural network (and many other machine learning methods) is a *linear transformation layer*. Going back to our `y = w*x + b` example, the `w*x + b` is the linear transformation: given an `x`, dot it with some `w`eights and add a `b`ias.\n\n**Task**: **Write a function that performs a linear transformation of a vector `x`.** Use PyTorch's built-in functionality for dot products (`torch.dot()` or ` @`).","metadata":{}},{"cell_type":"code","source":"def linear(weights, bias, x):\n    return (weights @ x) + bias\nlinear(w, -4.0, x)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:54:20.405710Z","iopub.execute_input":"2024-01-26T14:54:20.406147Z","iopub.status.idle":"2024-01-26T14:54:20.413199Z","shell.execute_reply.started":"2024-01-26T14:54:20.406112Z","shell.execute_reply":"2024-01-26T14:54:20.412413Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"tensor(2.)"},"metadata":{}}]},{"cell_type":"code","source":"assert torch.isclose(linear(w, -4.0, x), torch.tensor(2.0))\nassert torch.isclose(linear(w, 0.0, x), torch.tensor(6.0))","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:54:24.340976Z","iopub.execute_input":"2024-01-26T14:54:24.341385Z","iopub.status.idle":"2024-01-26T14:54:24.347670Z","shell.execute_reply.started":"2024-01-26T14:54:24.341348Z","shell.execute_reply":"2024-01-26T14:54:24.346504Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"\n\n### Linear layer, Module-style\n\nNotice that `linear`'s job is to transform `x`, but it needed 3 parameters, not just 1. It would be convenient to view the `linear` function as simply a function of `x`, with `weights` and `bias` being internal details.\n\nOne way to do this is to make a `Linear` *class* that has these as parameters.\n\n**Task**: Fill in the blanks in the template code to do this. (This is roughly how PyTorch's implementation works).","metadata":{}},{"cell_type":"code","source":"class Linear:\n    def __init__(self, weights, bias):\n        self.weights = weights\n        self.bias = bias\n        \n    def forward(self, x):\n        return (self.weights @ x) + self.bias\n\nlayer = Linear(weights=w, bias=1.0)\nlayer.forward(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:55:57.698732Z","iopub.execute_input":"2024-01-26T14:55:57.699120Z","iopub.status.idle":"2024-01-26T14:55:57.708990Z","shell.execute_reply.started":"2024-01-26T14:55:57.699090Z","shell.execute_reply":"2024-01-26T14:55:57.707988Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"tensor(7.)"},"metadata":{}}]},{"cell_type":"markdown","source":"Note: PyTorch's [`Linear` layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) gives a vector-valued output, so to make the dimensionality work out, it actually computes `x @ weights.T + bias`, where `T` computes the transpose of the array.","metadata":{}},{"cell_type":"markdown","source":"\n## Mean Squared Error\n\nNow let's apply what you just learned about elementwise operations on PyTorch tensors to another very common building block in machine learning: measuring *error*.\n\nOnce we make some predictions, we usually want to be able to measure how *good* the predictions were. For regression tasks, i.e., tasks where we're predicting *numbers*, one very common measure is the *mean squared error*. Here's an algorithm to compute it:\n\n- compute `resid` as true (`y_true`) minus predicted (`y_pred`).\n- compute `squared_error` by squaring each number in `resid`\n- compute `mean_squared_error` by taking the `mean` of `squared_error`.\n\n> **Technical note**: This process implements the mean squared error *loss function*. That is a function that is given some *true* values (call them $y_1$ through $y_n$) and some *predicted* values (call them $\\hat{y}_1$ through $\\hat{y}_n$) and returns $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2.$$\n\nGenerally you'd get the predicted values, `y_pred`, by calling a function that implements a model (like `linear.forward()` above. But to focus our attention on the error computation, we've provided sample values for `y_true` and `y_pred` that you can just use as-is.","metadata":{}},{"cell_type":"code","source":"y_true = torch.tensor([3.14, 1.59, 2.65])\ny_pred = torch.tensor([2.71, 8.28, 1.83])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:02:32.601768Z","iopub.execute_input":"2024-01-26T15:02:32.602664Z","iopub.status.idle":"2024-01-26T15:02:32.608949Z","shell.execute_reply.started":"2024-01-26T15:02:32.602613Z","shell.execute_reply":"2024-01-26T15:02:32.607373Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"**Task**:\n\n1. Implement each line of the above algorithm in PyTorch code.\n    - Use separate cells so you can check the results along the way. For example, the first cell should have two lines, the first to assign (`resid = ...`) and the second to show the result (`resid`).\n    - **You should not need to write any loops.**\n    - Try using both `squared_error.mean()` and `torch.mean(squared_error)`.\n2. Now, write the entire computation in a single succinct expression (i.e., without having to create intermediate variables for `resid` and `squared_error`). Check that you get the same result.\n\n> Notes:\n> \n> - Recall that Python's exponentiation operator is `**`.\n> - PyTorch tensors also have a `.pow()` method. So you can also use `.pow(2)`; you might see this in other people's code.\n","metadata":{}},{"cell_type":"code","source":"resid = torch.sub(y_true, y_pred)\nresid","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:02:34.720137Z","iopub.execute_input":"2024-01-26T15:02:34.720518Z","iopub.status.idle":"2024-01-26T15:02:34.728956Z","shell.execute_reply.started":"2024-01-26T15:02:34.720488Z","shell.execute_reply":"2024-01-26T15:02:34.727659Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.4300, -6.6900,  0.8200])"},"metadata":{}}]},{"cell_type":"code","source":"squared_error = torch.square(resid)\nsquared_error","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:02:37.674946Z","iopub.execute_input":"2024-01-26T15:02:37.675356Z","iopub.status.idle":"2024-01-26T15:02:37.689627Z","shell.execute_reply.started":"2024-01-26T15:02:37.675325Z","shell.execute_reply":"2024-01-26T15:02:37.688397Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.1849, 44.7561,  0.6724])"},"metadata":{}}]},{"cell_type":"code","source":"mean_squared_error = torch.mean(squared_error)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:04:22.384689Z","iopub.execute_input":"2024-01-26T15:04:22.385119Z","iopub.status.idle":"2024-01-26T15:04:22.390507Z","shell.execute_reply.started":"2024-01-26T15:04:22.385085Z","shell.execute_reply":"2024-01-26T15:04:22.389365Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:04:24.687326Z","iopub.execute_input":"2024-01-26T15:04:24.688412Z","iopub.status.idle":"2024-01-26T15:04:24.695839Z","shell.execute_reply.started":"2024-01-26T15:04:24.688370Z","shell.execute_reply":"2024-01-26T15:04:24.694759Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"tensor(15.2045)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Multidimensional arrays\n\nNumPy / PyTorch arrays can have more than one axis. Think of these like lists of lists (of lists of lists of ...).","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1234)\nx = torch.rand(4, 2)\nx","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:04:32.444432Z","iopub.execute_input":"2024-01-26T15:04:32.444867Z","iopub.status.idle":"2024-01-26T15:04:32.461128Z","shell.execute_reply.started":"2024-01-26T15:04:32.444832Z","shell.execute_reply":"2024-01-26T15:04:32.460231Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"tensor([[0.0290, 0.4019],\n        [0.2598, 0.3666],\n        [0.0583, 0.7006],\n        [0.0518, 0.4681]])"},"metadata":{}}]},{"cell_type":"markdown","source":"**Task**: Use *indexing* to get out the top-left number, the top-right number, the bottom-left, and the bottom-right. One of them is done for you; study how that works.","metadata":{}},{"cell_type":"code","source":"bottom_right = x[-1, -1]\nassert bottom_right == x[3, 1] and bottom_right == x[3][-1]\nbottom_right","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:05:16.869789Z","iopub.execute_input":"2024-01-26T15:05:16.870209Z","iopub.status.idle":"2024-01-26T15:05:16.878981Z","shell.execute_reply.started":"2024-01-26T15:05:16.870176Z","shell.execute_reply":"2024-01-26T15:05:16.877924Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"tensor(0.4681)"},"metadata":{}}]},{"cell_type":"code","source":"# Correct Values: Top-Left: 0.03, Top-Right: 0.40, Bottom-Left: 0.05\ntop_left = x[0,0]\ntop_right = x[0,1]\nbottom_left = x[-1,0]\nprint(f\"Top-Left: {top_left:.2f}, Top-Right: {top_right:.2f}, Bottom-Left: {bottom_left:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:07:16.972994Z","iopub.execute_input":"2024-01-26T15:07:16.973393Z","iopub.status.idle":"2024-01-26T15:07:16.979254Z","shell.execute_reply.started":"2024-01-26T15:07:16.973364Z","shell.execute_reply":"2024-01-26T15:07:16.978068Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Top-Left: 0.03, Top-Right: 0.40, Bottom-Left: 0.05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can apply a reduction operation \"along\" an axis, e.g.,","metadata":{}},{"cell_type":"code","source":"x.sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:08:06.089417Z","iopub.execute_input":"2024-01-26T15:08:06.089818Z","iopub.status.idle":"2024-01-26T15:08:06.098101Z","shell.execute_reply.started":"2024-01-26T15:08:06.089786Z","shell.execute_reply":"2024-01-26T15:08:06.097131Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"tensor([0.4309, 0.6265, 0.7589, 0.5199])"},"metadata":{}}]},{"cell_type":"markdown","source":"*Task*: Is summing on `axis=1` summing each row or summing each column?","metadata":{}},{"cell_type":"markdown","source":"**Answer**: The summing on ```axis=1``` is summing each row together. For example, in row 0, it is adding up 0.0290 and 0.4019. That adds up to 0.4309. In addition, there are 4 values that the line ```x.sum(axis=1)``` outputs. There are also 4 rows in x. So, that means the rows are being added together.","metadata":{}},{"cell_type":"markdown","source":"There's a general rule for what happens when you reduce along an axis: that axis \"goes away\". To think about that rule and its implications, try the following exercise:\n\n**Task**: Predict what the `.shape` of each of the following operations will be. Then try each one and check if you were correct. For example, for the first operation, `z.max(axis=0)`, the shape should be `(6, 7)`; check that it's true and make sure you can explain why.","metadata":{}},{"cell_type":"markdown","source":"Finally, the tensor product is a reduction operation that happens between two arrays / tensors. It reduces \"along\" the middle axis.\n\n**Task**: Try to find several different shapes that make the following code succeed.","metadata":{}},{"cell_type":"code","source":"shape1 = (2, 3, 4) # try to find examples with 1, 2, or 3 different numbers here.\nshape2 = (4, 2) # try to find examples with 1, 2, or 3 different numbers here.\nx = torch.rand(shape1)\ny = torch.rand(shape2)\n(x @ y).shape","metadata":{},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 2])"]},"metadata":{}}]},{"cell_type":"code","source":"#(rows, columns)\nz = torch.rand(5, 6, 7)\nz.shape\n\n# The shape should be (6,7)\n#z.mean(axis=0)\n\n# The shape should be (5, 7)\n#z.mean(axis=1)\n\n#The shape should be (5,6)\n#z.mean(axis=2)\n\n#The shape should also be (5,6) because it's taking in the last value of the array\n#z.mean(axis=-1)\n\n#The shape should be similar to the z.mean(axis=0), so(6,7)\n#z[0]  # indexing is kind of like a reduction operation\n\n#FIGURE OUT WHY THE OUTPUT IS LIKE THIS\nz[1].max(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:24:44.982075Z","iopub.execute_input":"2024-01-26T15:24:44.983409Z","iopub.status.idle":"2024-01-26T15:24:44.992119Z","shell.execute_reply.started":"2024-01-26T15:24:44.983356Z","shell.execute_reply":"2024-01-26T15:24:44.991250Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"torch.return_types.max(\nvalues=tensor([0.9429, 0.9082, 0.8151, 0.8475, 0.9871, 0.9283]),\nindices=tensor([5, 0, 3, 5, 2, 2]))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Appendix\n\nFor comparison and future reference, here's PyTorch's internal implementation of MSE loss. There are two ways to access it: the [functional style](https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss)...","metadata":{}},{"cell_type":"code","source":"F.mse_loss(y_pred, y_true)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:29:09.261979Z","iopub.execute_input":"2024-01-26T15:29:09.262375Z","iopub.status.idle":"2024-01-26T15:29:09.274637Z","shell.execute_reply.started":"2024-01-26T15:29:09.262343Z","shell.execute_reply":"2024-01-26T15:29:09.273552Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"tensor(15.2045)"},"metadata":{}}]},{"cell_type":"markdown","source":"and the [module style](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss):","metadata":{}},{"cell_type":"code","source":"loss_fn = nn.MSELoss()\nloss_fn(y_pred, y_true)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T15:29:12.786432Z","iopub.execute_input":"2024-01-26T15:29:12.786829Z","iopub.status.idle":"2024-01-26T15:29:12.795625Z","shell.execute_reply.started":"2024-01-26T15:29:12.786795Z","shell.execute_reply":"2024-01-26T15:29:12.794490Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"tensor(15.2045)"},"metadata":{}}]}]}